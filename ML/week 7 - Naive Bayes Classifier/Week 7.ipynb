{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a8e1d174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26\n",
      "Probability that a student with an A grade is a hosteler: 0.692\n",
      "0.029700000000000004\n",
      "Probability that a person has a disease given the test is positive is : 0.333\n"
     ]
    }
   ],
   "source": [
    "## Q1 solve the question using bayes theorum and then code the same\n",
    "\n",
    "# a part\n",
    "# given\n",
    "P_H = 0.60\n",
    "P_D = 0.40\n",
    "P_A_given_H = 0.30\n",
    "P_A_given_D = 0.20\n",
    "\n",
    "# probability of A grade\n",
    "P_A = (P_A_given_H * P_H) + (P_A_given_D * P_D)\n",
    "\n",
    "# bayes' theorem to find P(H | A)\n",
    "P_H_given_A = (P_A_given_H * P_H) / P_A\n",
    "\n",
    "print(f\"Probability that a student with an A grade is a hosteler: {P_H_given_A:.3f}\")\n",
    "\n",
    "# b part\n",
    "# given\n",
    "P_D = 0.01\n",
    "P_not_D = 0.99\n",
    "P_pos_given_D = 0.99\n",
    "P_pos_given_not_D = 0.02\n",
    "\n",
    "\n",
    "# prob of p(D|pos)\n",
    "P_D_given_pos = (P_pos_given_D * P_D) / (\n",
    "    (P_pos_given_D * P_D) + (P_pos_given_not_D * P_not_D)\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Probability that a person has a disease given the test is positive is : {P_D_given_pos:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93e1b995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: {'No': 0.5714285714285714, 'Yes': 0.4285714285714286}\n"
     ]
    }
   ],
   "source": [
    "## Q2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_probs = {}  # basically the output values probability\n",
    "        self.feature_probs = {}  # the input features\n",
    "        self.classes = []\n",
    "        self.features = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # here x is a dataframe and y is a series, hence we get all the unique values for outputs\n",
    "        self.classes = np.unique(y)\n",
    "        self.features = X.columns  # get features\n",
    "\n",
    "        # we take the average probability of the output\n",
    "        self.class_probs = {cls: np.mean(y == cls) for cls in self.classes}\n",
    "\n",
    "        # creating a suitable dict structure\n",
    "        self.feature_probs = {\n",
    "            cls: {feature: {} for feature in self.features} for cls in self.classes\n",
    "        }\n",
    "\n",
    "        # creating the conditional probability\n",
    "        for cls in self.classes:\n",
    "            class_data = X[y == cls]  # gets the features for a certain output class\n",
    "            for feature in self.features:\n",
    "                feature_data = class_data[feature]\n",
    "                values, counts = np.unique(\n",
    "                    feature_data, return_counts=True\n",
    "                )  # gets all the unique values and returns an array of the frequncy\n",
    "                probs = dict(\n",
    "                    zip(values, counts / len(feature_data))\n",
    "                )  # convert to probability and place in a temp dict\n",
    "                self.feature_probs[cls][feature] = probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        # the math behind the working, we get the proportionality not the actual value\n",
    "        # P(Ck∣F1,F2,…,Fn)∝P(Ck)⋅[P(F1∣Ck)⋅P(F2∣Ck)⋅…⋅P(Fn∣Ck)]\n",
    "        for _, row in X.iterrows():\n",
    "            class_probs = {}\n",
    "            for cls in self.classes:\n",
    "                prob = self.class_probs[cls]\n",
    "                for feature in self.features:\n",
    "                    value = row[feature]\n",
    "                    prob *= self.feature_probs[cls].get(feature).get(value)\n",
    "                class_probs[cls] = prob\n",
    "\n",
    "        return class_probs\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"buy_computer.csv\")\n",
    "\n",
    "# split features\n",
    "X = data.drop(\"Buys Computer\", axis=1)\n",
    "y = data[\"Buys Computer\"]\n",
    "\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(X, y)\n",
    "\n",
    "# Example prediction\n",
    "test_data = pd.DataFrame(\n",
    "    {\"Age\": [\"<=30\"], \"Income\": [\"High\"], \"Student\": [\"Yes\"], \"Credit Rating\": [\"Fair\"]}\n",
    ")\n",
    "\n",
    "# predict\n",
    "# normalising the predictions we can get the actual probability\n",
    "predictions = nb_classifier.predict(test_data)\n",
    "total = 0\n",
    "for i in predictions.keys():\n",
    "    total += predictions.get(i)\n",
    "res = {}\n",
    "for i in predictions.keys():\n",
    "    res[i] = predictions.get(i) / total\n",
    "\n",
    "print(f\"Predictions: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "271a5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with sample data\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"The game was exciting and intense\",\n",
    "        \"The match ended with a close score\",\n",
    "        \"The team won the championship\",\n",
    "        \"The weather was sunny and warm\",\n",
    "        \"She went to the beach and relaxed\",\n",
    "        \"They watched a movie together\",\n",
    "        \"The player scored a last-minute goal\",\n",
    "        \"He enjoyed a delicious meal\",\n",
    "    ],\n",
    "    \"label\": [\n",
    "        \"sports\",\n",
    "        \"sports\",\n",
    "        \"sports\",\n",
    "        \"not sports\",\n",
    "        \"not sports\",\n",
    "        \"not sports\",\n",
    "        \"sports\",\n",
    "        \"not sports\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"text_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1a86d9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for ['A very close game']: {'not sports': 2.999991000027e-06, 'sports': 0.999997000009}\n"
     ]
    }
   ],
   "source": [
    "# Q3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_probs = {}\n",
    "        self.feature_probs = {}\n",
    "        self.classes = []\n",
    "        self.feature_names = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.feature_names = X.columns\n",
    "\n",
    "        self.class_probs = {cls: np.mean(y == cls) for cls in self.classes}\n",
    "\n",
    "        self.feature_probs = {\n",
    "            cls: {feature: {} for feature in self.feature_names} for cls in self.classes\n",
    "        }\n",
    "\n",
    "        for cls in self.classes:\n",
    "            class_data = X[y == cls]\n",
    "            for feature in self.feature_names:\n",
    "                feature_data = class_data[feature]\n",
    "                values, counts = np.unique(feature_data, return_counts=True)\n",
    "                probs = dict(zip(values, counts / len(feature_data)))\n",
    "                self.feature_probs[cls][feature] = probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for _, row in X.iterrows():\n",
    "            class_probs = {}\n",
    "            for cls in self.classes:\n",
    "                prob = self.class_probs[cls]\n",
    "                for feature in self.feature_names:\n",
    "                    value = row[feature]\n",
    "                    prob *= self.feature_probs[cls].get(feature, {}).get(value, 1e-6)\n",
    "                class_probs[cls] = prob\n",
    "            predictions.append(max(class_probs, key=class_probs.get))\n",
    "        return class_probs\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"text_data.csv\")\n",
    "\n",
    "# vectorize the text data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data[\"text\"]).toarray()\n",
    "X = pd.DataFrame(X, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "y = data[\"label\"]\n",
    "\n",
    "\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(X, y)\n",
    "\n",
    "# new sentence\n",
    "new_sentence = [\"A very close game\"]\n",
    "X_new = vectorizer.transform(new_sentence).toarray()\n",
    "X_new = pd.DataFrame(X_new, columns=vectorizer.get_feature_names_out())\n",
    "predictions = nb_classifier.predict(X_new)\n",
    "# print(\"cool till here\")\n",
    "res = {}\n",
    "total = 0\n",
    "for i in prediction.keys():\n",
    "    total += predictions.get(i)\n",
    "\n",
    "for i in prediction.keys():\n",
    "    res[i] = prediction.get(i) / total\n",
    "\n",
    "print(f\"Prediction for {new_sentence}: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f730890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
