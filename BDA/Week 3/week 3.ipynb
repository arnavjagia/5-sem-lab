{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code mostly copied from chapter 2 of _Advanced Analytics with PySpark_ \n",
    "\n",
    "To download the dataset \n",
    "```shell\n",
    "$ mkdir linkage\n",
    "$ cd linkage/\n",
    "$ curl -L -o donation.zip https://bit.ly/1Aoywaq\n",
    "$ unzip donation.zip\n",
    "$ unzip 'block_*.zip'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/24 01:09:16 WARN Utils: Your hostname, Ambujs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.86.0.75 instead (on interface en0)\n",
      "24/08/24 01:09:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/24 01:09:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = 'data/linkage/block*.csv'\n",
    "df = spark.read.csv(path, header=True, nullValue='?', inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache the DataFrame\n",
    "\n",
    "[\"Explaining the mechanics of Spark caching\"](https://luminousmen.com/post/explaining-the-mechanics-of-spark-caching) -- great article explaining why and how Spark DataFrames are cached\n",
    "\n",
    "The first action run after `df.cache()` will take its time (including overhead of caching). All subsequent actions will be fast it seems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:==============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.59 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "55.5 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.cache()\n",
    "\n",
    "%timeit -n 1 -r 1 df.count() # slow\n",
    "%timeit -n 1 -r 1 df.count() # fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create summary statistics for matches and non-matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def pivot(source_df):\n",
    "    \"\"\"Return a pivoted summary of `source_df`.\"\"\"\n",
    "\n",
    "    # convert to pandas DataFrame\n",
    "    pd_df = source_df.toPandas()\n",
    "\n",
    "    # pivot\n",
    "    pd_df = pd_df.set_index('summary').transpose().reset_index()\n",
    "    pd_df = pd_df.rename(columns={'index':'field'})\n",
    "\n",
    "    # convert to Spark DataFrame\n",
    "    result_df = spark.createDataFrame(pd_df)\n",
    "\n",
    "    # convert non-index columns from string to double\n",
    "    for c in result_df.columns:\n",
    "        if c == 'field':\n",
    "            continue\n",
    "        result_df = result_df.withColumn(c, col(c).cast('double'))\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/24 01:09:26 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+------------------+--------------------+---+-------+\n",
      "|       field|  count|              mean|              stddev|min|    max|\n",
      "+------------+-------+------------------+--------------------+---+-------+\n",
      "|        id_1|20931.0| 34575.72117911232|   21950.31285196913|5.0|99946.0|\n",
      "|        id_2|20931.0| 51259.95939037791|   24345.73345377519|6.0|99996.0|\n",
      "|cmp_fname_c1|20922.0|0.9973163859635038| 0.03650667584833679|0.0|    1.0|\n",
      "|cmp_fname_c2| 1333.0|0.9898900320318176| 0.08251973727615237|0.0|    1.0|\n",
      "|cmp_lname_c1|20931.0|0.9970152595958817|0.043118807533945126|0.0|    1.0|\n",
      "|cmp_lname_c2|  475.0| 0.969370167843852| 0.15345280740388917|0.0|    1.0|\n",
      "|     cmp_sex|20931.0| 0.987291577086618| 0.11201570591216435|0.0|    1.0|\n",
      "|      cmp_bd|20925.0|0.9970848267622461| 0.05391487659807981|0.0|    1.0|\n",
      "|      cmp_bm|20925.0|0.9979450418160095|0.045286127452170664|0.0|    1.0|\n",
      "|      cmp_by|20925.0|0.9961290322580645| 0.06209804856731055|0.0|    1.0|\n",
      "|     cmp_plz|20902.0|0.9584250310975027| 0.19962063345931919|0.0|    1.0|\n",
      "+------------+-------+------------------+--------------------+---+-------+\n",
      "\n",
      "+------------+---------+--------------------+--------------------+----+--------+\n",
      "|       field|    count|                mean|              stddev| min|     max|\n",
      "+------------+---------+--------------------+--------------------+----+--------+\n",
      "|        id_1|5728201.0|  33319.913548075565|  23665.760130330676| 1.0| 99980.0|\n",
      "|        id_2|5728201.0|   66643.44259218557|  23599.551728241313|30.0|100000.0|\n",
      "|cmp_fname_c1|5727203.0|  0.7118634802175091| 0.38908060096985553| 0.0|     1.0|\n",
      "|cmp_fname_c2| 102365.0|  0.8988473514090158| 0.27272090294010215| 0.0|     1.0|\n",
      "|cmp_lname_c1|5728201.0|  0.3131380113364304|  0.3322812130572686| 0.0|     1.0|\n",
      "|cmp_lname_c2|   1989.0| 0.16295544855122532|  0.1930236663528703| 0.0|     1.0|\n",
      "|     cmp_sex|5728201.0|  0.9548833918362851| 0.20755988859217375| 0.0|     1.0|\n",
      "|      cmp_bd|5727412.0|  0.2216425149788421|  0.4153518275558732| 0.0|     1.0|\n",
      "|      cmp_bm|5727412.0|   0.486995347986141|  0.4998308940493865| 0.0|     1.0|\n",
      "|      cmp_by|5727412.0|  0.2199230647280133|  0.4141943267142977| 0.0|     1.0|\n",
      "|     cmp_plz|5715387.0|0.002043781112285135|0.045161979893625206| 0.0|     1.0|\n",
      "+------------+---------+--------------------+--------------------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "match_stats = pivot(df.where(df.is_match == True).describe())\n",
    "miss_stats = pivot(df.where(df.is_match == False).describe())\n",
    "\n",
    "match_stats.show()\n",
    "miss_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute how much each field differs for matches versus non-matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------------+\n",
      "|       field|occurances|               delta|\n",
      "+------------+----------+--------------------+\n",
      "|     cmp_plz| 5736289.0|  0.9563812499852176|\n",
      "|cmp_lname_c2|    2464.0|  0.8064147192926266|\n",
      "|      cmp_by| 5748337.0|  0.7762059675300512|\n",
      "|      cmp_bd| 5748337.0|   0.775442311783404|\n",
      "|cmp_lname_c1| 5749132.0|  0.6838772482594513|\n",
      "|      cmp_bm| 5748337.0|  0.5109496938298685|\n",
      "|cmp_fname_c1| 5748125.0|  0.2854529057459947|\n",
      "|cmp_fname_c2|  103698.0| 0.09104268062280174|\n",
      "|     cmp_sex| 5749132.0|0.032408185250332844|\n",
      "+------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "match_stats.createOrReplaceTempView('match_stats')\n",
    "miss_stats.createOrReplaceTempView('miss_stats')\n",
    "\n",
    "query = '''\n",
    "    select A.field, A.count + B.count as occurances, A.mean - B.mean as delta\n",
    "    from match_stats A \n",
    "        inner join miss_stats B on A.field = B.field\n",
    "    where A.field not in ('id_1', 'id_2')\n",
    "    order by delta desc;\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "First select relevant features\n",
    "\n",
    "What makes a good feature?\n",
    "- Has different values for matches and non-matches\n",
    "- Available most of the time (not null for most instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|score|is_match|\n",
      "+-----+--------+\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  4.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  4.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "scored = df.fillna(0).\\\n",
    "            withColumn('score', expr(\" + \".join(features))).\\\n",
    "            select('score', 'is_match')\n",
    "\n",
    "scored.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate(threshold):\n",
    "    scored.selectExpr(f'score >= {threshold} as above', 'is_match').\\\n",
    "        groupBy('above').pivot('is_match', ('true', 'false')).\\\n",
    "        count().\\\n",
    "        show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 2\n",
      "+-----+-----+-------+\n",
      "|above| true|  false|\n",
      "+-----+-----+-------+\n",
      "| true|20931| 596414|\n",
      "|false| NULL|5131787|\n",
      "+-----+-----+-------+\n",
      "\n",
      "threshold = 3\n",
      "+-----+-----+-------+\n",
      "|above| true|  false|\n",
      "+-----+-----+-------+\n",
      "| true|20916| 315213|\n",
      "|false|   15|5412988|\n",
      "+-----+-----+-------+\n",
      "\n",
      "threshold = 4\n",
      "+-----+-----+-------+\n",
      "|above| true|  false|\n",
      "+-----+-----+-------+\n",
      "| true|20871|    637|\n",
      "|false|   60|5727564|\n",
      "+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(2, 5):\n",
    "    print(f'threshold = {t}')\n",
    "    tabulate(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
